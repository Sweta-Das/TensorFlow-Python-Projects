{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZzvQT2qiyYYp1o7/yoIrP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sweta-Das/TensorFlow-Python-Projects/blob/Fundamentals/2-Linear_Regression/optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimization\n",
        "- It is the process of adjusting the parameters (weights and biases) of a model to minimize (or maximize) a certain objective function (like loss function).\n",
        "- Goal of optimization is to minimize the loss so that the model becomes more accurate."
      ],
      "metadata": {
        "id": "UrtLOMt78TlU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importance of Optimization in Training Linear Regression Model\n",
        "- Optimization is crucial for training the linear regression model as the model's accuracy directly depends on finding the best values of $w$ and $b$.\n",
        "- Poor optimization can lead to underfitting or overfitting.\n",
        "\n",
        "### Steps to Optimize Linear Regression Model\n",
        "1. **Initializing parameters** -> Assigning random values for $w$ and $b$\n",
        "2. **Forward Pass** -> Computing the predictions $ŷ$\n",
        "3. **Calculate loss** -> Using a loss function to measure the error between $ŷ$ and $y$\n",
        "4. **Backpropagation** -> Computing the gradients of the loss function w.r.t $w$ and $b$\n",
        "5. **Update Parameters** -> Adjusting $w$ and $b$ using an optimization algo. like *Gradient Descent*\n",
        "6. The process is repeated until the loss converges to a minimum or reaches a stopping criterion."
      ],
      "metadata": {
        "id": "HhXZDZ2e8vHO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bZ6-foJw8N1L"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating synthetic data for training & testing\n",
        "np.random.seed(42)\n",
        "X_train = np.random.rand(100, 1) # 100 samples, 1 feature\n",
        "y_train = 4 * X_train + np.random.randn(100, 1) # Linear relation with some noise\n",
        "\n",
        "X_test = np.random.rand(20, 1)\n",
        "y_test = 4 * X_test + np.random.randn(20, 1)"
      ],
      "metadata": {
        "id": "2iJQ8Pcb-pcN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating linear regression model in tensorflow\n",
        "class LinearRegressionModel(tf.Module):\n",
        "  def __init__(self):\n",
        "    # Initializing weight and bias randomly\n",
        "    self.w = tf.Variable(tf.random.normal([1]), name=\"weight\")\n",
        "    self.b = tf.Variable(tf.random.normal([1]), name=\"bias\")\n",
        "\n",
        "  def __call__(self, X):\n",
        "    return self.w * X + self.b\n",
        "\n",
        "model = LinearRegressionModel()"
      ],
      "metadata": {
        "id": "B74wVv5p-8q6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss Function (Mean Squared Error)\n",
        "def loss_fn(y_true, y_pred):\n",
        "  return tf.reduce_mean(tf.square(y_true - y_pred))"
      ],
      "metadata": {
        "id": "06agWdTQ_cbU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "def train(model, X_train, y_train, epochs):\n",
        "  # Initializing optimizer after model creation\n",
        "  optimizer = tf.optimizers.SGD(learning_rate=0.1)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = model(X_train)\n",
        "      loss = loss_fn(y_train, predictions)\n",
        "\n",
        "    # Computing gradients and updating parameters\n",
        "    gradients = tape.gradient(loss, [model.w, model.b])\n",
        "    optimizer.apply_gradients(zip(gradients, [model.w, model.b]))\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "      print(f\"Epoch {epoch}: Loss = {loss.numpy():.4f}, w = {model.w.numpy()}, b = {model.b.numpy()}\")\n",
        "\n",
        "train(model, X_train, y_train, epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa1J0HyqAAtE",
        "outputId": "5b3235be-316b-45ba-feec-52c639613dd5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss = 4.5440, w = [0.25911975], b = [0.5524367]\n",
            "Epoch 10: Loss = 1.3363, w = [1.1261972], b = [1.3596457]\n",
            "Epoch 20: Loss = 1.1972, w = [1.4720178], b = [1.2571218]\n",
            "Epoch 30: Loss = 1.1003, w = [1.7482758], b = [1.121551]\n",
            "Epoch 40: Loss = 1.0274, w = [1.9864544], b = [1.0012794]\n",
            "Epoch 50: Loss = 0.9726, w = [2.1929066], b = [0.89683014]\n",
            "Epoch 60: Loss = 0.9314, w = [2.3719232], b = [0.80624974]\n",
            "Epoch 70: Loss = 0.9005, w = [2.527154], b = [0.72770417]\n",
            "Epoch 80: Loss = 0.8772, w = [2.6617596], b = [0.65959466]\n",
            "Epoch 90: Loss = 0.8597, w = [2.77848], b = [0.6005348]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "\n",
        "# Defining accuracy function (MAE and RMSE)\n",
        "def mean_abs_error(y_true, y_pred):\n",
        "  return tf.reduce_mean(tf.abs(y_true-y_pred))\n",
        "\n",
        "def root_mean_sqr_error(y_true, y_pred):\n",
        "  return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
        "\n",
        "def test(model, X_test, y_test):\n",
        "  predictions = model(X_test)\n",
        "  loss = loss_fn(y_test, predictions)\n",
        "  mae = mean_abs_error(y_test, predictions)\n",
        "  rmse = root_mean_sqr_error(y_test, predictions)\n",
        "  print(f\"Test MAE: {mae.numpy(): .4f}, Test RMSE: {rmse.numpy():.4f}, Test Loss: {loss.numpy():.4f}\")\n",
        "  return mae, rmse"
      ],
      "metadata": {
        "id": "UlaOX85aAro6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(model=model, X_test=X_test, y_test=y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fN6vROxQDA9D",
        "outputId": "ee19c240-a65f-4b39-d5a0-adf1af0bd05a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MAE:  0.6896, Test RMSE: 0.8188, Test Loss: 0.6704\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(), dtype=float32, numpy=0.6896304>,\n",
              " <tf.Tensor: shape=(), dtype=float32, numpy=0.8187523>)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - MAE = 0.6896 means that, on average, the model is relatively close to the actual test values but still has room for improvement.\n",
        " - RMSE = 0.8188 means that there are some larger errors in predictions, though the errors are relatively moderate.\n",
        " - Test loss = 0.6704 is consistent with the RMSE, suggesting a reasonable fit, though improvements could be made."
      ],
      "metadata": {
        "id": "Xgk1Dn_qDsvP"
      }
    }
  ]
}