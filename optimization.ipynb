{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sweta-Das/TensorFlow-Python-Projects/blob/Fundamentals/optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrtLOMt78TlU"
      },
      "source": [
        "# Optimization\n",
        "- It is the process of adjusting the parameters (weights and biases) of a model to minimize (or maximize) a certain objective function (like loss function).\n",
        "- Goal of optimization is to minimize the loss so that the model becomes more accurate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhXZDZ2e8vHO"
      },
      "source": [
        "## Importance of Optimization in Training Linear Regression Model\n",
        "- Optimization is crucial for training the linear regression model as the model's accuracy directly depends on finding the best values of $w$ and $b$.\n",
        "- Poor optimization can lead to underfitting or overfitting.\n",
        "\n",
        "### Steps to Optimize Linear Regression Model\n",
        "1. **Initializing parameters** -> Assigning random values for $w$ and $b$\n",
        "2. **Forward Pass** -> Computing the predictions $ŷ$\n",
        "3. **Calculate loss** -> Using a loss function to measure the error between $ŷ$ and $y$\n",
        "4. **Backpropagation** -> Computing the gradients of the loss function w.r.t $w$ and $b$\n",
        "5. **Update Parameters** -> Adjusting $w$ and $b$ using an optimization algo. like *Gradient Descent*\n",
        "6. The process is repeated until the loss converges to a minimum or reaches a stopping criterion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZ6-foJw8N1L"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iJQ8Pcb-pcN"
      },
      "outputs": [],
      "source": [
        "# Generating synthetic data for training & testing\n",
        "np.random.seed(42)\n",
        "X_train = np.random.rand(100, 1) # 100 samples, 1 feature\n",
        "y_train = 4 * X_train + np.random.randn(100, 1) # Linear relation with some noise\n",
        "\n",
        "X_test = np.random.rand(20, 1)\n",
        "y_test = 4 * X_test + np.random.randn(20, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B74wVv5p-8q6"
      },
      "outputs": [],
      "source": [
        "# Creating linear regression model in tensorflow\n",
        "class LinearRegressionModel(tf.Module):\n",
        "  def __init__(self):\n",
        "    # Initializing weight and bias randomly\n",
        "    self.w = tf.Variable(tf.random.normal([1]), name=\"weight\")\n",
        "    self.b = tf.Variable(tf.random.normal([1]), name=\"bias\")\n",
        "\n",
        "  def __call__(self, X):\n",
        "    return self.w * X + self.b\n",
        "\n",
        "model = LinearRegressionModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06agWdTQ_cbU"
      },
      "outputs": [],
      "source": [
        "# Loss Function (Mean Squared Error)\n",
        "def loss_fn(y_true, y_pred):\n",
        "  return tf.reduce_mean(tf.square(y_true - y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa1J0HyqAAtE",
        "outputId": "5b3235be-316b-45ba-feec-52c639613dd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: Loss = 4.5440, w = [0.25911975], b = [0.5524367]\n",
            "Epoch 10: Loss = 1.3363, w = [1.1261972], b = [1.3596457]\n",
            "Epoch 20: Loss = 1.1972, w = [1.4720178], b = [1.2571218]\n",
            "Epoch 30: Loss = 1.1003, w = [1.7482758], b = [1.121551]\n",
            "Epoch 40: Loss = 1.0274, w = [1.9864544], b = [1.0012794]\n",
            "Epoch 50: Loss = 0.9726, w = [2.1929066], b = [0.89683014]\n",
            "Epoch 60: Loss = 0.9314, w = [2.3719232], b = [0.80624974]\n",
            "Epoch 70: Loss = 0.9005, w = [2.527154], b = [0.72770417]\n",
            "Epoch 80: Loss = 0.8772, w = [2.6617596], b = [0.65959466]\n",
            "Epoch 90: Loss = 0.8597, w = [2.77848], b = [0.6005348]\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "def train(model, X_train, y_train, epochs):\n",
        "  # Initializing optimizer after model creation\n",
        "  optimizer = tf.optimizers.SGD(learning_rate=0.1)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = model(X_train)\n",
        "      loss = loss_fn(y_train, predictions)\n",
        "\n",
        "    # Computing gradients and updating parameters\n",
        "    gradients = tape.gradient(loss, [model.w, model.b])\n",
        "    optimizer.apply_gradients(zip(gradients, [model.w, model.b]))\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "      print(f\"Epoch {epoch}: Loss = {loss.numpy():.4f}, w = {model.w.numpy()}, b = {model.b.numpy()}\")\n",
        "\n",
        "train(model, X_train, y_train, epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlaOX85aAro6"
      },
      "outputs": [],
      "source": [
        "# Testing\n",
        "\n",
        "# Defining accuracy function (MAE and RMSE)\n",
        "def mean_abs_error(y_true, y_pred):\n",
        "  return tf.reduce_mean(tf.abs(y_true-y_pred))\n",
        "\n",
        "def root_mean_sqr_error(y_true, y_pred):\n",
        "  return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
        "\n",
        "def test(model, X_test, y_test):\n",
        "  predictions = model(X_test)\n",
        "  loss = loss_fn(y_test, predictions)\n",
        "  mae = mean_abs_error(y_test, predictions)\n",
        "  rmse = root_mean_sqr_error(y_test, predictions)\n",
        "  print(f\"Test MAE: {mae.numpy(): .4f}, Test RMSE: {rmse.numpy():.4f}, Test Loss: {loss.numpy():.4f}\")\n",
        "  return mae, rmse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fN6vROxQDA9D",
        "outputId": "ee19c240-a65f-4b39-d5a0-adf1af0bd05a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test MAE:  0.6896, Test RMSE: 0.8188, Test Loss: 0.6704\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(), dtype=float32, numpy=0.6896304>,\n",
              " <tf.Tensor: shape=(), dtype=float32, numpy=0.8187523>)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test(model=model, X_test=X_test, y_test=y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xgk1Dn_qDsvP"
      },
      "source": [
        " - MAE = 0.6896 means that, on average, the model is relatively close to the actual test values but still has room for improvement.\n",
        " - RMSE = 0.8188 means that there are some larger errors in predictions, though the errors are relatively moderate.\n",
        " - Test loss = 0.6704 is consistent with the RMSE, suggesting a reasonable fit, though improvements could be made."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HpcH1Cs0GN9"
      },
      "source": [
        "## Gradient Descent\n",
        "\n",
        "- an optimization algorithm i.e. used to minimize the cost function (or loss function) in ML models, particularly in deep learning and neural networks.\n",
        "- Its main goal is to adjust the model's parameters (weights & biases) iteratively by moving in the direction that reduces the error or cost most rapidly.\n",
        "- Process of gradient descent involves;\n",
        "    - Computing the gradient (partial derivatives) of the cost function concerning each parameter.\n",
        "    - Updating the parameters in the opposite direction of the gradient to minimize the cost function.\n",
        "    - Repeating the process until the cost function reaches a minimum.\n",
        "- Formula to update each parameter: <br>\n",
        "\n",
        "    $\\theta = \\theta - \\eta . \\frac{\\partial J(\\theta)}{\\partial \\theta}$\n",
        "    Here,\n",
        "    - $\\eta$ = learning rate; a small constant determining the step size for each iteration\n",
        "    - $\\frac{\\partial J(\\theta)}{\\partial \\theta}$ = gradient of the cost function $J(\\theta)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Jhh-YLt0GN9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating synthetic data\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(100, 1).astype(np.float32) # 100 data points as inputs\n",
        "y_true = 3.5 * X + 1.2 + np.random.randn(100, 1) * 0.1 # y=3.5*X + 1.2 with some noise"
      ],
      "metadata": {
        "id": "CN60cK7U0ZEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the linear model: y = w * X + b\n",
        "class LinearModel(tf.Module):\n",
        "  def __init__(self):\n",
        "    self.w = tf.Variable(np.random.randn(), dtype=tf.float32)\n",
        "    self.b = tf.Variable(np.random.randn(), dtype=tf.float32)\n",
        "\n",
        "  def __call__(self, X):\n",
        "    return self.w * X + self.b"
      ],
      "metadata": {
        "id": "dtDivxmk0yyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the loss function (Mean Squared Error)\n",
        "def loss_fn(y_pred, y_true):\n",
        "  return tf.reduce_mean(tf.square(y_pred - y_true))"
      ],
      "metadata": {
        "id": "9XKmmlwF1J0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model using Gradient Descent\n",
        "def train_step(model, X, y_true, learning_rate):\n",
        "  with tf.GradientTape() as tape:\n",
        "    y_pred = model(X)\n",
        "    loss = loss_fn(y_pred, y_true)\n",
        "  gradients = tape.gradient(loss, [model.w, model.b])\n",
        "  model.w.assign_sub(learning_rate * gradients[0])\n",
        "  model.b.assign_sub(learning_rate * gradients[1])\n",
        "  return loss"
      ],
      "metadata": {
        "id": "OoMUU9xj1TTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing model\n",
        "model = LinearModel()\n",
        "\n",
        "# Training parameters\n",
        "learning_rate = 0.1\n",
        "epochs = 500\n",
        "\n",
        "# Training the model\n",
        "for epoch in range(epochs):\n",
        "  loss = train_step(model, X, y_true, learning_rate)\n",
        "  if epoch % 50 == 0: # Printing the loss every 50 epochs\n",
        "    print(f\"Epoch {epoch}: Loss = {loss.numpy(): .4f}, w: {model.w.numpy():.4f}, b: {model.b.numpy():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqF2d6Sy1pfI",
        "outputId": "79a18846-ffb9-4f83-eb84-573602fe3b92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss =  30.6112, w: -0.0641, b: -1.1431\n",
            "Epoch 50: Loss =  0.0890, w: 2.5380, b: 1.7065\n",
            "Epoch 100: Loss =  0.0304, w: 3.0071, b: 1.4688\n",
            "Epoch 150: Loss =  0.0152, w: 3.2460, b: 1.3478\n",
            "Epoch 200: Loss =  0.0113, w: 3.3676, b: 1.2861\n",
            "Epoch 250: Loss =  0.0103, w: 3.4295, b: 1.2548\n",
            "Epoch 300: Loss =  0.0100, w: 3.4610, b: 1.2388\n",
            "Epoch 350: Loss =  0.0099, w: 3.4771, b: 1.2306\n",
            "Epoch 400: Loss =  0.0099, w: 3.4852, b: 1.2265\n",
            "Epoch 450: Loss =  0.0099, w: 3.4894, b: 1.2244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final learned parameters\n",
        "print(f\"\\nFinal parameters: w = {model.w.numpy():.4f}, b = {model.b.numpy():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfo4sQaB2d08",
        "outputId": "b4be000f-7927-4fba-b6aa-7d7062083223"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final parameters: w = 3.4915, b = 1.2233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYkld6yf0GN9"
      },
      "source": [
        "### Types of Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5CqqYGb0GN-"
      },
      "source": [
        "**1. Batch Gradient Descent (BGD)**\n",
        "- Here, the gradient of the cost function is computed using the entire dataset for each iteration.\n",
        "- It is a deterministic method, as it computes the average gradient of all training examples.\n",
        "\n",
        "**Advantages**\n",
        "- Smooth convergence to the global or local minimum.\n",
        "- Computationally efficient when dealing with smaller datasets.\n",
        "\n",
        "**Disadvantages**\n",
        "- For large datasets, computing the gradient over the entire dataset at each step can be slow and memory-intensive.\n",
        "- May be infeasible for very large datasets.\n",
        "\n",
        "**Update Rule**\n",
        "\n",
        "$\\theta = \\theta - \\eta . \\frac{1}{m} \\sum_{i=1}^m \\nabla_\\theta J (\\theta, x^{(i)}, y^{(i)})$ <br>\n",
        "\n",
        "where, m = number of training examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XB89p5a10GN-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4a6c1bc-c11e-4793-ca7f-4ba4bbde8033"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss = 9.431061744689941\n",
            "Epoch 100: Loss = 0.02597496658563614\n",
            "Epoch 200: Loss = 0.009100452065467834\n",
            "Epoch 300: Loss = 0.008125617168843746\n",
            "Epoch 400: Loss = 0.008069302886724472\n",
            "Epoch 500: Loss = 0.00806605163961649\n",
            "Epoch 600: Loss = 0.008065860718488693\n",
            "Epoch 700: Loss = 0.008065851405262947\n",
            "Epoch 800: Loss = 0.008065850473940372\n",
            "Epoch 900: Loss = 0.008065846748650074\n",
            "Final parameters: w = [[2.954013]], b = [2.0215147]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Generating sample data\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) # 100 samples, 1 feature\n",
        "y = 3*X+2 + np.random.randn(100, 1)*0.1 # y = 3x + 2 + noise\n",
        "\n",
        "# Converting data to TensorFlow tensors\n",
        "X_train = tf.constant(X, dtype=tf.float32)\n",
        "y_train = tf.constant(y, dtype=tf.float32)\n",
        "\n",
        "# Defining model parameters\n",
        "weights = tf.Variable(tf.random.normal([1, 1]), dtype=tf.float32)\n",
        "bias = tf.Variable(tf.random.normal([1]), dtype=tf.float32)\n",
        "\n",
        "# Defining hyperparameters\n",
        "learning_rate = 0.1\n",
        "epochs = 1000\n",
        "\n",
        "# Training loop for batch gradient descent\n",
        "for epoch in range(epochs):\n",
        "  with tf.GradientTape() as tape:\n",
        "    # Linear model: y_pred = X*weights + bias\n",
        "    y_pred = tf.matmul(X_train, weights) + bias\n",
        "\n",
        "    # Mean Squared Error Loss\n",
        "    loss = tf.reduce_mean(tf.square(y_train - y_pred))\n",
        "\n",
        "  # Computing gradients\n",
        "  gradients = tape.gradient(loss, [weights, bias])\n",
        "\n",
        "  # Updating parameters\n",
        "  weights.assign_sub(learning_rate * gradients[0])\n",
        "  bias.assign_sub(learning_rate * gradients[1])\n",
        "\n",
        "  # Logging progress every 100 epochs\n",
        "  if epoch % 100 == 0:\n",
        "    print(f\"Epoch {epoch}: Loss = {loss.numpy()}\")\n",
        "\n",
        "# Output final parameters\n",
        "print(f\"Final parameters: w = {weights.numpy()}, b = {bias.numpy()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, a synthetic data is generated for a simple linear regression problem, `y=3x + 2 + noise`. The linear model is defined as `y_pred = X*weights + bias` and MSE is used to measure the error. `tf.GradientTape` computes the gradients of the loss w.r.t the parameters. Gradients are applied to update `weights` & `bias` using the learning rate.<br>\n",
        "\n",
        "We can see that the loss is decreasing over epochs and the trained parameters (`weights` & `bias`) approaches the true values 3 & 2 respectively."
      ],
      "metadata": {
        "id": "DpaIhG4_x97h"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLhiP52j0GN-"
      },
      "source": [
        "**2. Stochastic Gradient Descent (SGD)**\n",
        "- Here, the gradient of the cost function is computed for each training example individually, making faster updates and learning before the entire dataset has been processed.\n",
        "\n",
        "**Advantages**\n",
        "- Faster iterations as parameters are updated for each training example.\n",
        "- Can handle large datasets as it processes one example at a time.\n",
        "- Often escapes local minima due to inherent randomness.\n",
        "\n",
        "**Disadvantages**\n",
        "- Convergence path is noisier that Batch Gradient Descent, making it harder to achieve convergence to the minimum.\n",
        "- May overshoot the minimum, causing fluctuation around it.\n",
        "\n",
        "**Update Rule**\n",
        "\n",
        "$\\theta = \\theta - \\eta . \\nabla_\\theta J(\\theta, x^{(i)}, y^{(i)})$ <br>\n",
        "\n",
        "where, $x^{(i)}, y^{(i)}$ represents a single training example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgBa-LkZ0GN-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnGT9ip10GN-"
      },
      "source": [
        "**3. Mini-Batch Gradient Descent (MBGD)**\n",
        "- Provides a balance between Batch & Stochastic Gradient Descent.\n",
        "- Instead of using the entire dataset (BGD) or one sample (SGD), Mini-Batch Gradient Descent updates the parameters by computing the gradient based on a small batch of training examples.\n",
        "\n",
        "**Advantages**\n",
        "- Provides benefits of both BGD and SGD: faster than BGD and more stable that SGD.\n",
        "- Efficiently utilizes vectorized operations and hardware acceleration (e.g., GPUs)\n",
        "- Noise in gradient updates helps avoid local minima while the batch size stabilizes the updates.\n",
        "\n",
        "**Disadvantages**\n",
        "- Requires tuning the batch size, which can affect convergence speed and performance.\n",
        "\n",
        "**Update Rule**\n",
        "\n",
        "$\\theta = \\theta - \\eta . \\frac{1}{n} \\sum_{i=1}^n \\nabla_{\\theta}J(\\theta, x^{(i)}, y^{(i)})$\n",
        "\n",
        "where, $n$ = mini-batch size (e.g., 32, 64, 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ABOMbGq0GN-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}